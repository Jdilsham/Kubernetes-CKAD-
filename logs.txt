
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMMAND â”‚     ARGS     â”‚ PROFILE  â”‚      USER       â”‚ VERSION â”‚      START TIME       â”‚ END TIME â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start   â”‚ --cni=calico â”‚ minikube â”‚ janitha-dilsham â”‚ v1.37.0 â”‚ 28 Oct 25 23:39 +0530 â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2025/10/28 23:39:40
Running on machine: janitha-dilsham-ASUS-EXPERTBOOK-B1503CVA
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1028 23:39:40.740111    9672 out.go:360] Setting OutFile to fd 1 ...
I1028 23:39:40.740673    9672 out.go:413] isatty.IsTerminal(1) = true
I1028 23:39:40.740684    9672 out.go:374] Setting ErrFile to fd 2...
I1028 23:39:40.740690    9672 out.go:413] isatty.IsTerminal(2) = true
I1028 23:39:40.740942    9672 root.go:338] Updating PATH: /home/janitha-dilsham/.minikube/bin
W1028 23:39:40.741190    9672 root.go:314] Error reading config file at /home/janitha-dilsham/.minikube/config/config.json: open /home/janitha-dilsham/.minikube/config/config.json: no such file or directory
I1028 23:39:40.741852    9672 out.go:368] Setting JSON to false
I1028 23:39:40.743682    9672 start.go:130] hostinfo: {"hostname":"janitha-dilsham-ASUS-EXPERTBOOK-B1503CVA","uptime":2081,"bootTime":1761672900,"procs":346,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.14.0-33-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"577651d1-06c4-496e-95d9-378f9ec90b59"}
I1028 23:39:40.743906    9672 start.go:140] virtualization: kvm host
I1028 23:39:40.745063    9672 out.go:179] ðŸ˜„  minikube v1.37.0 on Ubuntu 24.04
I1028 23:39:40.746809    9672 notify.go:220] Checking for updates...
I1028 23:39:40.747182    9672 driver.go:421] Setting default libvirt URI to qemu:///system
W1028 23:39:40.747183    9672 preload.go:293] Failed to list preload files: open /home/janitha-dilsham/.minikube/cache/preloaded-tarball: no such file or directory
I1028 23:39:40.747204    9672 global.go:112] Querying for installed drivers using PATH=/home/janitha-dilsham/.minikube/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
I1028 23:39:40.747275    9672 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1028 23:39:40.747321    9672 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1028 23:39:40.804355    9672 docker.go:123] docker version: linux-28.5.1:Docker Engine - Community
I1028 23:39:40.804468    9672 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1028 23:39:40.933330    9672 info.go:266] docker info: {ID:52f46ffe-43c6-4da7-abfa-279ae50fa353 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-10-28 23:39:40.924555095 +0530 +0530 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-33-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24766210048 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:janitha-dilsham-ASUS-EXPERTBOOK-B1503CVA Labels:[] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:75cb2b7193e4e490e9fbdc236c0e811ccaba3376 Expected:} RuncCommit:{ID:v1.3.2-0-gaeabe4e7 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2]] Warnings:<nil>}}
I1028 23:39:40.933457    9672 docker.go:318] overlay module found
I1028 23:39:40.933482    9672 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1028 23:39:40.943282    9672 global.go:133] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1028 23:39:40.943424    9672 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1028 23:39:40.943448    9672 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1028 23:39:40.943527    9672 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1028 23:39:40.943595    9672 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1028 23:39:40.943625    9672 driver.go:343] not recommending "none" due to default: false
I1028 23:39:40.943630    9672 driver.go:343] not recommending "ssh" due to default: false
I1028 23:39:40.943644    9672 driver.go:378] Picked: docker
I1028 23:39:40.943652    9672 driver.go:379] Alternatives: [none ssh]
I1028 23:39:40.943658    9672 driver.go:380] Rejects: [virtualbox vmware podman kvm2 qemu2]
I1028 23:39:40.945944    9672 out.go:179] âœ¨  Automatically selected the docker driver. Other choices: none, ssh
I1028 23:39:40.947272    9672 start.go:304] selected driver: docker
I1028 23:39:40.947282    9672 start.go:918] validating driver "docker" against <nil>
I1028 23:39:40.947297    9672 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1028 23:39:40.947390    9672 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1028 23:39:40.999832    9672 info.go:266] docker info: {ID:52f46ffe-43c6-4da7-abfa-279ae50fa353 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-10-28 23:39:40.990410331 +0530 +0530 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-33-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24766210048 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:janitha-dilsham-ASUS-EXPERTBOOK-B1503CVA Labels:[] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:75cb2b7193e4e490e9fbdc236c0e811ccaba3376 Expected:} RuncCommit:{ID:v1.3.2-0-gaeabe4e7 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2]] Warnings:<nil>}}
I1028 23:39:41.000008    9672 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1028 23:39:41.001351    9672 start_flags.go:410] Using suggested 5900MB memory alloc based on sys=23618MB, container=23618MB
I1028 23:39:41.001551    9672 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1028 23:39:41.002575    9672 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I1028 23:39:41.003298    9672 cni.go:84] Creating CNI manager for "calico"
I1028 23:39:41.003307    9672 start_flags.go:336] Found "Calico" CNI - setting NetworkPlugin=cni
I1028 23:39:41.003413    9672 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1028 23:39:41.004404    9672 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1028 23:39:41.005930    9672 cache.go:123] Beginning downloading kic base image for docker with docker
I1028 23:39:41.006679    9672 out.go:179] ðŸšœ  Pulling base image v0.0.48 ...
I1028 23:39:41.008141    9672 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1028 23:39:41.008219    9672 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1028 23:39:41.026773    9672 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1028 23:39:41.027117    9672 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1028 23:39:41.027219    9672 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1028 23:39:41.706397    9672 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1028 23:39:41.706422    9672 cache.go:58] Caching tarball of preloaded images
I1028 23:39:41.706715    9672 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1028 23:39:41.710545    9672 out.go:179] ðŸ’¾  Downloading Kubernetes v1.34.0 preload ...
I1028 23:39:41.711642    9672 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1028 23:39:42.319768    9672 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/janitha-dilsham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1028 23:48:41.690373    9672 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1028 23:48:41.690407    9672 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1028 23:48:51.528816    9672 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1028 23:50:02.288405    9672 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1028 23:50:02.288454    9672 preload.go:254] verifying checksum of /home/janitha-dilsham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1028 23:50:02.868280    9672 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1028 23:50:02.868497    9672 profile.go:143] Saving config to /home/janitha-dilsham/.minikube/profiles/minikube/config.json ...
I1028 23:50:02.868513    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/config.json: {Name:mkc198ec1d21aeaf557d0306ee8c1c6ee2619c28 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:02.868631    9672 cache.go:232] Successfully downloaded all kic artifacts
I1028 23:50:02.868641    9672 start.go:360] acquireMachinesLock for minikube: {Name:mkb13436b1470e884b53e78670ac7449fed5c6f2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1028 23:50:02.868664    9672 start.go:364] duration metric: took 17.845Âµs to acquireMachinesLock for "minikube"
I1028 23:50:02.868674    9672 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1028 23:50:02.868701    9672 start.go:125] createHost starting for "" (driver="docker")
I1028 23:50:02.905437    9672 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=5900MB) ...
I1028 23:50:02.906159    9672 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1028 23:50:02.906212    9672 client.go:168] LocalClient.Create starting
I1028 23:50:02.906484    9672 main.go:141] libmachine: Creating CA: /home/janitha-dilsham/.minikube/certs/ca.pem
I1028 23:50:02.952814    9672 main.go:141] libmachine: Creating client certificate: /home/janitha-dilsham/.minikube/certs/cert.pem
I1028 23:50:03.090472    9672 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1028 23:50:03.104245    9672 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1028 23:50:03.104271    9672 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1028 23:50:03.104292    9672 cli_runner.go:164] Run: docker network inspect minikube
W1028 23:50:03.115234    9672 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1028 23:50:03.115256    9672 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1028 23:50:03.115268    9672 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1028 23:50:03.115344    9672 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1028 23:50:03.131666    9672 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0003a39c0}
I1028 23:50:03.131716    9672 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1028 23:50:03.131767    9672 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1028 23:50:03.200003    9672 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1028 23:50:03.200024    9672 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1028 23:50:03.200081    9672 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1028 23:50:03.216998    9672 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1028 23:50:03.238020    9672 oci.go:103] Successfully created a docker volume minikube
I1028 23:50:03.238087    9672 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1028 23:50:03.576741    9672 oci.go:107] Successfully prepared a docker volume minikube
I1028 23:50:03.576773    9672 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1028 23:50:03.576788    9672 kic.go:194] Starting extracting preloaded images to volume ...
I1028 23:50:03.576825    9672 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/janitha-dilsham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1028 23:50:06.131031    9672 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/janitha-dilsham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (2.554151688s)
I1028 23:50:06.131058    9672 kic.go:203] duration metric: took 2.554264586s to extract preloaded images to volume ...
W1028 23:50:06.131166    9672 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1028 23:50:06.131213    9672 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1028 23:50:06.131254    9672 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1028 23:50:06.175110    9672 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=5900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1028 23:50:06.501272    9672 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1028 23:50:06.520696    9672 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1028 23:50:06.541082    9672 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1028 23:50:06.602891    9672 oci.go:144] the created container "minikube" has a running status.
I1028 23:50:06.602916    9672 kic.go:225] Creating ssh key for kic: /home/janitha-dilsham/.minikube/machines/minikube/id_rsa...
I1028 23:50:06.710026    9672 kic_runner.go:191] docker (temp): /home/janitha-dilsham/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1028 23:50:06.727727    9672 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1028 23:50:06.742761    9672 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1028 23:50:06.742774    9672 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1028 23:50:06.797152    9672 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1028 23:50:06.817131    9672 machine.go:93] provisionDockerMachine start ...
I1028 23:50:06.817228    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:06.837553    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:06.837832    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:06.837841    9672 main.go:141] libmachine: About to run SSH command:
hostname
I1028 23:50:06.838704    9672 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39658->127.0.0.1:32768: read: connection reset by peer
I1028 23:50:09.991205    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1028 23:50:09.991225    9672 ubuntu.go:182] provisioning hostname "minikube"
I1028 23:50:09.991292    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:10.013180    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:10.013450    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:10.013460    9672 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1028 23:50:10.173590    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1028 23:50:10.173666    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:10.194574    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:10.194830    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:10.194847    9672 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1028 23:50:10.348643    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1028 23:50:10.348663    9672 ubuntu.go:188] set auth options {CertDir:/home/janitha-dilsham/.minikube CaCertPath:/home/janitha-dilsham/.minikube/certs/ca.pem CaPrivateKeyPath:/home/janitha-dilsham/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/janitha-dilsham/.minikube/machines/server.pem ServerKeyPath:/home/janitha-dilsham/.minikube/machines/server-key.pem ClientKeyPath:/home/janitha-dilsham/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/janitha-dilsham/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/janitha-dilsham/.minikube}
I1028 23:50:10.348680    9672 ubuntu.go:190] setting up certificates
I1028 23:50:10.348688    9672 provision.go:84] configureAuth start
I1028 23:50:10.348743    9672 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1028 23:50:10.369200    9672 provision.go:143] copyHostCerts
I1028 23:50:10.369277    9672 exec_runner.go:151] cp: /home/janitha-dilsham/.minikube/certs/ca.pem --> /home/janitha-dilsham/.minikube/ca.pem (1103 bytes)
I1028 23:50:10.369429    9672 exec_runner.go:151] cp: /home/janitha-dilsham/.minikube/certs/cert.pem --> /home/janitha-dilsham/.minikube/cert.pem (1147 bytes)
I1028 23:50:10.369534    9672 exec_runner.go:151] cp: /home/janitha-dilsham/.minikube/certs/key.pem --> /home/janitha-dilsham/.minikube/key.pem (1675 bytes)
I1028 23:50:10.369605    9672 provision.go:117] generating server cert: /home/janitha-dilsham/.minikube/machines/server.pem ca-key=/home/janitha-dilsham/.minikube/certs/ca.pem private-key=/home/janitha-dilsham/.minikube/certs/ca-key.pem org=janitha-dilsham.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1028 23:50:10.596975    9672 provision.go:177] copyRemoteCerts
I1028 23:50:10.597009    9672 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1028 23:50:10.597034    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:10.607490    9672 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/janitha-dilsham/.minikube/machines/minikube/id_rsa Username:docker}
I1028 23:50:10.708307    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I1028 23:50:10.742731    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1028 23:50:10.771260    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1103 bytes)
I1028 23:50:10.798635    9672 provision.go:87] duration metric: took 449.936866ms to configureAuth
I1028 23:50:10.798652    9672 ubuntu.go:206] setting minikube options for container-runtime
I1028 23:50:10.798793    9672 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1028 23:50:10.798831    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:10.816153    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:10.816391    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:10.816400    9672 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1028 23:50:10.964900    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1028 23:50:10.964916    9672 ubuntu.go:71] root file system type: overlay
I1028 23:50:10.965044    9672 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1028 23:50:10.965097    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:10.982638    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:10.982882    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:10.982990    9672 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1028 23:50:11.148558    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1028 23:50:11.148671    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:11.164097    9672 main.go:141] libmachine: Using SSH client type: native
I1028 23:50:11.164465    9672 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1028 23:50:11.164486    9672 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1028 23:50:12.594752    9672 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-10-28 18:20:11.145499696 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1028 23:50:12.594772    9672 machine.go:96] duration metric: took 5.777627173s to provisionDockerMachine
I1028 23:50:12.594784    9672 client.go:171] duration metric: took 9.688564915s to LocalClient.Create
I1028 23:50:12.594802    9672 start.go:167] duration metric: took 9.688656704s to libmachine.API.Create "minikube"
I1028 23:50:12.594810    9672 start.go:293] postStartSetup for "minikube" (driver="docker")
I1028 23:50:12.594822    9672 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1028 23:50:12.594884    9672 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1028 23:50:12.594932    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:12.615263    9672 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/janitha-dilsham/.minikube/machines/minikube/id_rsa Username:docker}
I1028 23:50:12.726285    9672 ssh_runner.go:195] Run: cat /etc/os-release
I1028 23:50:12.730231    9672 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1028 23:50:12.730256    9672 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1028 23:50:12.730265    9672 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1028 23:50:12.730271    9672 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1028 23:50:12.730278    9672 filesync.go:126] Scanning /home/janitha-dilsham/.minikube/addons for local assets ...
I1028 23:50:12.730343    9672 filesync.go:126] Scanning /home/janitha-dilsham/.minikube/files for local assets ...
I1028 23:50:12.730361    9672 start.go:296] duration metric: took 135.546031ms for postStartSetup
I1028 23:50:12.730687    9672 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1028 23:50:12.748445    9672 profile.go:143] Saving config to /home/janitha-dilsham/.minikube/profiles/minikube/config.json ...
I1028 23:50:12.748679    9672 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1028 23:50:12.748708    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:12.767606    9672 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/janitha-dilsham/.minikube/machines/minikube/id_rsa Username:docker}
I1028 23:50:12.870155    9672 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1028 23:50:12.875748    9672 start.go:128] duration metric: took 10.00703706s to createHost
I1028 23:50:12.875760    9672 start.go:83] releasing machines lock for "minikube", held for 10.00709056s
I1028 23:50:12.875824    9672 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1028 23:50:12.894587    9672 ssh_runner.go:195] Run: cat /version.json
I1028 23:50:12.894632    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:12.894639    9672 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1028 23:50:12.894688    9672 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1028 23:50:12.914053    9672 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/janitha-dilsham/.minikube/machines/minikube/id_rsa Username:docker}
I1028 23:50:12.914615    9672 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/janitha-dilsham/.minikube/machines/minikube/id_rsa Username:docker}
I1028 23:50:13.543914    9672 ssh_runner.go:195] Run: systemctl --version
I1028 23:50:13.550358    9672 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1028 23:50:13.555902    9672 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1028 23:50:13.591147    9672 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1028 23:50:13.591214    9672 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1028 23:50:13.621850    9672 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1028 23:50:13.621865    9672 start.go:495] detecting cgroup driver to use...
I1028 23:50:13.621896    9672 detect.go:190] detected "systemd" cgroup driver on host os
I1028 23:50:13.622155    9672 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1028 23:50:13.640334    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1028 23:50:13.654578    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1028 23:50:13.666593    9672 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1028 23:50:13.666631    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1028 23:50:13.679218    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1028 23:50:13.690294    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1028 23:50:13.701563    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1028 23:50:13.711607    9672 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1028 23:50:13.720973    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1028 23:50:13.731523    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1028 23:50:13.740163    9672 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1028 23:50:13.750212    9672 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1028 23:50:13.758803    9672 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1028 23:50:13.766861    9672 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1028 23:50:13.820250    9672 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1028 23:50:13.900734    9672 start.go:495] detecting cgroup driver to use...
I1028 23:50:13.900764    9672 detect.go:190] detected "systemd" cgroup driver on host os
I1028 23:50:13.900799    9672 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1028 23:50:13.913866    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1028 23:50:13.927156    9672 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1028 23:50:13.943896    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1028 23:50:13.957746    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1028 23:50:13.970116    9672 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1028 23:50:13.988886    9672 ssh_runner.go:195] Run: which cri-dockerd
I1028 23:50:13.992959    9672 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1028 23:50:14.004218    9672 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1028 23:50:14.024950    9672 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1028 23:50:14.080347    9672 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1028 23:50:14.131326    9672 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1028 23:50:14.131373    9672 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1028 23:50:14.146828    9672 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1028 23:50:14.158566    9672 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1028 23:50:14.215940    9672 ssh_runner.go:195] Run: sudo systemctl restart docker
I1028 23:50:15.308633    9672 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.092670835s)
I1028 23:50:15.308693    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1028 23:50:15.322178    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1028 23:50:15.335525    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1028 23:50:15.348120    9672 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1028 23:50:15.409387    9672 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1028 23:50:15.463265    9672 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1028 23:50:15.501383    9672 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1028 23:50:15.522251    9672 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1028 23:50:15.536027    9672 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1028 23:50:15.598622    9672 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1028 23:50:15.676695    9672 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1028 23:50:15.696208    9672 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1028 23:50:15.696394    9672 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1028 23:50:15.703166    9672 start.go:563] Will wait 60s for crictl version
I1028 23:50:15.703267    9672 ssh_runner.go:195] Run: which crictl
I1028 23:50:15.709679    9672 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1028 23:50:15.750184    9672 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1028 23:50:15.750213    9672 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1028 23:50:15.777358    9672 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1028 23:50:15.805766    9672 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1028 23:50:15.805878    9672 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1028 23:50:15.825961    9672 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1028 23:50:15.830905    9672 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1028 23:50:15.843947    9672 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1028 23:50:15.844045    9672 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1028 23:50:15.844095    9672 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1028 23:50:15.865557    9672 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1028 23:50:15.865568    9672 docker.go:621] Images already preloaded, skipping extraction
I1028 23:50:15.865616    9672 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1028 23:50:15.886377    9672 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1028 23:50:15.886391    9672 cache_images.go:85] Images are preloaded, skipping loading
I1028 23:50:15.886401    9672 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1028 23:50:15.886510    9672 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico}
I1028 23:50:15.886568    9672 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1028 23:50:15.939547    9672 cni.go:84] Creating CNI manager for "calico"
I1028 23:50:15.939565    9672 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1028 23:50:15.939592    9672 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1028 23:50:15.939701    9672 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1028 23:50:15.939751    9672 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1028 23:50:15.950567    9672 binaries.go:44] Found k8s binaries, skipping transfer
I1028 23:50:15.950618    9672 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1028 23:50:15.960150    9672 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1028 23:50:15.979494    9672 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1028 23:50:15.999350    9672 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1028 23:50:16.019015    9672 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1028 23:50:16.022873    9672 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1028 23:50:16.034434    9672 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1028 23:50:16.093524    9672 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1028 23:50:16.112358    9672 certs.go:68] Setting up /home/janitha-dilsham/.minikube/profiles/minikube for IP: 192.168.49.2
I1028 23:50:16.112368    9672 certs.go:194] generating shared ca certs ...
I1028 23:50:16.112382    9672 certs.go:226] acquiring lock for ca certs: {Name:mkea1ec2db77dbbb702884077ae0901ac93882d0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.112487    9672 certs.go:240] generating "minikubeCA" ca cert: /home/janitha-dilsham/.minikube/ca.key
I1028 23:50:16.533418    9672 crypto.go:156] Writing cert to /home/janitha-dilsham/.minikube/ca.crt ...
I1028 23:50:16.533429    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/ca.crt: {Name:mk71c63419e851758a8e55b139c69388c2c4acc1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.533554    9672 crypto.go:164] Writing key to /home/janitha-dilsham/.minikube/ca.key ...
I1028 23:50:16.533557    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/ca.key: {Name:mk658715afbc1f78e59ddc17e13a39bd53f6d245 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.533599    9672 certs.go:240] generating "proxyClientCA" ca cert: /home/janitha-dilsham/.minikube/proxy-client-ca.key
I1028 23:50:16.642805    9672 crypto.go:156] Writing cert to /home/janitha-dilsham/.minikube/proxy-client-ca.crt ...
I1028 23:50:16.642811    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/proxy-client-ca.crt: {Name:mk63e224ab3bdf3feb9a6f8fbfb76e5f7e1d517b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.642890    9672 crypto.go:164] Writing key to /home/janitha-dilsham/.minikube/proxy-client-ca.key ...
I1028 23:50:16.642892    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/proxy-client-ca.key: {Name:mk0fe44f156368eea306cf27a3ac7687e7f9e8e5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.642920    9672 certs.go:256] generating profile certs ...
I1028 23:50:16.642954    9672 certs.go:363] generating signed profile cert for "minikube-user": /home/janitha-dilsham/.minikube/profiles/minikube/client.key
I1028 23:50:16.642959    9672 crypto.go:68] Generating cert /home/janitha-dilsham/.minikube/profiles/minikube/client.crt with IP's: []
I1028 23:50:16.793975    9672 crypto.go:156] Writing cert to /home/janitha-dilsham/.minikube/profiles/minikube/client.crt ...
I1028 23:50:16.793982    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/client.crt: {Name:mkfc62de7c78de677c48fdd9475752860da51589 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.794053    9672 crypto.go:164] Writing key to /home/janitha-dilsham/.minikube/profiles/minikube/client.key ...
I1028 23:50:16.794056    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/client.key: {Name:mk7ca1ca83216400ac6ec9e73707a6749291b363 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.794085    9672 certs.go:363] generating signed profile cert for "minikube": /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1028 23:50:16.794092    9672 crypto.go:68] Generating cert /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1028 23:50:16.807257    9672 crypto.go:156] Writing cert to /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1028 23:50:16.807262    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk1aba9972da68bbc67d23496a066818264f4713 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.807302    9672 crypto.go:164] Writing key to /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1028 23:50:16.807304    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk1cb732c37e5398b78bb1352d740735e0c54c19 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:16.807333    9672 certs.go:381] copying /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt
I1028 23:50:16.807373    9672 certs.go:385] copying /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key
I1028 23:50:16.807392    9672 certs.go:363] generating signed profile cert for "aggregator": /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.key
I1028 23:50:16.807403    9672 crypto.go:68] Generating cert /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1028 23:50:17.047333    9672 crypto.go:156] Writing cert to /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.crt ...
I1028 23:50:17.047346    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.crt: {Name:mk02d07d9f559318141af077af18942d598671fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:17.047466    9672 crypto.go:164] Writing key to /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.key ...
I1028 23:50:17.047469    9672 lock.go:35] WriteFile acquiring /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.key: {Name:mkdc1d4810f57948a9d3509386d070d948dde92d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:17.047562    9672 certs.go:484] found cert: /home/janitha-dilsham/.minikube/certs/ca-key.pem (1675 bytes)
I1028 23:50:17.047579    9672 certs.go:484] found cert: /home/janitha-dilsham/.minikube/certs/ca.pem (1103 bytes)
I1028 23:50:17.047589    9672 certs.go:484] found cert: /home/janitha-dilsham/.minikube/certs/cert.pem (1147 bytes)
I1028 23:50:17.047599    9672 certs.go:484] found cert: /home/janitha-dilsham/.minikube/certs/key.pem (1675 bytes)
I1028 23:50:17.047959    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1028 23:50:17.070805    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1028 23:50:17.098085    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1028 23:50:17.124965    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1028 23:50:17.150404    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1028 23:50:17.174568    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1028 23:50:17.200569    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1028 23:50:17.227075    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1028 23:50:17.253125    9672 ssh_runner.go:362] scp /home/janitha-dilsham/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1028 23:50:17.285757    9672 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I1028 23:50:17.306232    9672 ssh_runner.go:195] Run: openssl version
I1028 23:50:17.312432    9672 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1028 23:50:17.325781    9672 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1028 23:50:17.329548    9672 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 28 18:20 /usr/share/ca-certificates/minikubeCA.pem
I1028 23:50:17.329586    9672 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1028 23:50:17.336868    9672 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1028 23:50:17.347461    9672 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1028 23:50:17.351106    9672 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1028 23:50:17.351148    9672 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1028 23:50:17.351247    9672 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1028 23:50:17.370062    9672 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1028 23:50:17.380234    9672 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1028 23:50:17.391209    9672 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1028 23:50:17.391243    9672 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1028 23:50:17.399932    9672 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1028 23:50:17.399940    9672 kubeadm.go:157] found existing configuration files:

I1028 23:50:17.399990    9672 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1028 23:50:17.410700    9672 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1028 23:50:17.410732    9672 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1028 23:50:17.419250    9672 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1028 23:50:17.428410    9672 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1028 23:50:17.428440    9672 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1028 23:50:17.437109    9672 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1028 23:50:17.445970    9672 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1028 23:50:17.446020    9672 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1028 23:50:17.454398    9672 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1028 23:50:17.463407    9672 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1028 23:50:17.463448    9672 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1028 23:50:17.472881    9672 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1028 23:50:17.506292    9672 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1028 23:50:17.506337    9672 kubeadm.go:310] [preflight] Running pre-flight checks
I1028 23:50:17.542014    9672 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1028 23:50:17.542080    9672 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.14.0-33-generic[0m
I1028 23:50:17.542111    9672 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1028 23:50:17.542154    9672 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1028 23:50:17.542196    9672 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1028 23:50:17.542250    9672 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1028 23:50:17.542297    9672 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1028 23:50:17.542345    9672 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1028 23:50:17.542397    9672 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1028 23:50:17.542441    9672 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1028 23:50:17.542476    9672 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1028 23:50:17.588793    9672 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1028 23:50:17.588996    9672 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1028 23:50:17.589127    9672 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1028 23:50:17.600440    9672 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1028 23:50:17.604685    9672 out.go:252]     â–ª Generating certificates and keys ...
I1028 23:50:17.604835    9672 kubeadm.go:310] [certs] Using existing ca certificate authority
I1028 23:50:17.604923    9672 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1028 23:50:17.763632    9672 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1028 23:50:18.147142    9672 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1028 23:50:18.226334    9672 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1028 23:50:18.398693    9672 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1028 23:50:18.513012    9672 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1028 23:50:18.513091    9672 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1028 23:50:18.559427    9672 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1028 23:50:18.559492    9672 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1028 23:50:18.700264    9672 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1028 23:50:18.850846    9672 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1028 23:50:18.967551    9672 kubeadm.go:310] [certs] Generating "sa" key and public key
I1028 23:50:18.967586    9672 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1028 23:50:19.041522    9672 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1028 23:50:19.208879    9672 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1028 23:50:19.307597    9672 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1028 23:50:19.383963    9672 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1028 23:50:19.636126    9672 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1028 23:50:19.636284    9672 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1028 23:50:19.642324    9672 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1028 23:50:19.643887    9672 out.go:252]     â–ª Booting up control plane ...
I1028 23:50:19.643973    9672 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1028 23:50:19.644045    9672 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1028 23:50:19.644086    9672 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1028 23:50:19.652002    9672 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1028 23:50:19.652077    9672 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1028 23:50:19.655861    9672 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1028 23:50:19.655909    9672 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1028 23:50:19.655929    9672 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1028 23:50:19.723722    9672 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1028 23:50:19.723916    9672 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1028 23:50:20.224942    9672 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.251953ms
I1028 23:50:20.230876    9672 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1028 23:50:20.231037    9672 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1028 23:50:20.231170    9672 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1028 23:50:20.231284    9672 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1028 23:50:21.365044    9672 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.134257043s
I1028 23:50:22.664133    9672 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.433307638s
I1028 23:50:24.232575    9672 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 4.001529104s
I1028 23:50:24.246324    9672 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1028 23:50:24.256061    9672 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1028 23:50:24.265096    9672 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1028 23:50:24.265462    9672 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1028 23:50:24.274602    9672 kubeadm.go:310] [bootstrap-token] Using token: zoq1ao.jsep2oiegtgo1pyl
I1028 23:50:24.275428    9672 out.go:252]     â–ª Configuring RBAC rules ...
I1028 23:50:24.275684    9672 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1028 23:50:24.279698    9672 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1028 23:50:24.285471    9672 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1028 23:50:24.288346    9672 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1028 23:50:24.291265    9672 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1028 23:50:24.294203    9672 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1028 23:50:24.644388    9672 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1028 23:50:25.058056    9672 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1028 23:50:25.642872    9672 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1028 23:50:25.644472    9672 kubeadm.go:310] 
I1028 23:50:25.644589    9672 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1028 23:50:25.644599    9672 kubeadm.go:310] 
I1028 23:50:25.644751    9672 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1028 23:50:25.644764    9672 kubeadm.go:310] 
I1028 23:50:25.644849    9672 kubeadm.go:310]   mkdir -p $HOME/.kube
I1028 23:50:25.644956    9672 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1028 23:50:25.645073    9672 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1028 23:50:25.645079    9672 kubeadm.go:310] 
I1028 23:50:25.645166    9672 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1028 23:50:25.645171    9672 kubeadm.go:310] 
I1028 23:50:25.645254    9672 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1028 23:50:25.645283    9672 kubeadm.go:310] 
I1028 23:50:25.645382    9672 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1028 23:50:25.645511    9672 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1028 23:50:25.645625    9672 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1028 23:50:25.645630    9672 kubeadm.go:310] 
I1028 23:50:25.645779    9672 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1028 23:50:25.645902    9672 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1028 23:50:25.645908    9672 kubeadm.go:310] 
I1028 23:50:25.646048    9672 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token zoq1ao.jsep2oiegtgo1pyl \
I1028 23:50:25.646215    9672 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:5326a24d92c583e225d2b6781de2b86eb70d57fbd7978043a71935201646d925 \
I1028 23:50:25.646246    9672 kubeadm.go:310] 	--control-plane 
I1028 23:50:25.646252    9672 kubeadm.go:310] 
I1028 23:50:25.646402    9672 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1028 23:50:25.646409    9672 kubeadm.go:310] 
I1028 23:50:25.646537    9672 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token zoq1ao.jsep2oiegtgo1pyl \
I1028 23:50:25.646692    9672 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:5326a24d92c583e225d2b6781de2b86eb70d57fbd7978043a71935201646d925 
I1028 23:50:25.649209    9672 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.14.0-33-generic\n", err: exit status 1
I1028 23:50:25.649353    9672 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1028 23:50:25.649383    9672 cni.go:84] Creating CNI manager for "calico"
I1028 23:50:25.650370    9672 out.go:179] ðŸ”—  Configuring Calico (Container Networking Interface) ...
I1028 23:50:25.652796    9672 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.34.0/kubectl ...
I1028 23:50:25.652808    9672 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (539470 bytes)
I1028 23:50:25.676714    9672 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1028 23:50:26.247155    9672 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1028 23:50:26.247192    9672 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1028 23:50:26.247217    9672 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_10_28T23_50_26_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1028 23:50:26.292575    9672 kubeadm.go:1105] duration metric: took 45.413799ms to wait for elevateKubeSystemPrivileges
I1028 23:50:26.292601    9672 ops.go:34] apiserver oom_adj: -16
I1028 23:50:26.292610    9672 kubeadm.go:394] duration metric: took 8.941467226s to StartCluster
I1028 23:50:26.292622    9672 settings.go:142] acquiring lock: {Name:mkdc227f3dce34e5a714ce6a4a0daa55c5d80456 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1028 23:50:26.292683    9672 settings.go:150] Updating kubeconfig:  /home/janitha-dilsham/.kube/config
I1028 23:50:26.293621    9672 out.go:203] 
W1028 23:50:26.294271    9672 out.go:285] âŒ  Exiting due to GUEST_START: failed to start node: Failed kubeconfig update: read kubeconfig from "/home/janitha-dilsham/.kube/config": read /home/janitha-dilsham/.kube/config: is a directory
W1028 23:50:26.294285    9672 out.go:285] 
W1028 23:50:26.295513    9672 out.go:308] [31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®[0m
[31mâ”‚[0m                                                                                           [31mâ”‚[0m
[31mâ”‚[0m    ðŸ˜¿  If the above advice does not help, please let us know:                             [31mâ”‚[0m
[31mâ”‚[0m    ðŸ‘‰  https://github.com/kubernetes/minikube/issues/new/choose                           [31mâ”‚[0m
[31mâ”‚[0m                                                                                           [31mâ”‚[0m
[31mâ”‚[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31mâ”‚[0m
[31mâ”‚[0m                                                                                           [31mâ”‚[0m
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯[0m
I1028 23:50:26.296260    9672 out.go:203] 


==> Docker <==
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.126 [INFO][48761] ipam/ipam_plugin.go 374: Released host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.126 [INFO][48761] ipam/ipam_plugin.go 283: Calico CNI IPAM assigned addresses IPv4=[10.244.120.65/26] IPv6=[] ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" HandleID="k8s-pod-network.bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Workload="minikube-k8s-coredns--66bc5c9577--vl47p-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.129 [INFO][48708] cni-plugin/k8s.go 418: Populated endpoint ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-coredns--66bc5c9577--vl47p-eth0", GenerateName:"coredns-66bc5c9577-", Namespace:"kube-system", SelfLink:"", UID:"8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6", ResourceVersion:"774", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-dns", "pod-template-hash":"66bc5c9577", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"coredns"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"", Pod:"coredns-66bc5c9577-vl47p", Endpoint:"eth0", ServiceAccountName:"coredns", IPNetworks:[]string{"10.244.120.65/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.coredns"}, InterfaceName:"cali39ccf043b1d", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"dns", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"UDP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"dns-tcp", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x23c1, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"liveness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f90, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"readiness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1ff5, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.129 [INFO][48708] cni-plugin/k8s.go 419: Calico CNI using IPs: [10.244.120.65/32] ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.130 [INFO][48708] cni-plugin/dataplane_linux.go 69: Setting the host side veth name to cali39ccf043b1d ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.132 [INFO][48708] cni-plugin/dataplane_linux.go 508: Disabling IPv4 forwarding ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.133 [INFO][48708] cni-plugin/k8s.go 446: Added Mac, interface name, and active container ID to endpoint ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-coredns--66bc5c9577--vl47p-eth0", GenerateName:"coredns-66bc5c9577-", Namespace:"kube-system", SelfLink:"", UID:"8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6", ResourceVersion:"774", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-dns", "pod-template-hash":"66bc5c9577", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"coredns"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070", Pod:"coredns-66bc5c9577-vl47p", Endpoint:"eth0", ServiceAccountName:"coredns", IPNetworks:[]string{"10.244.120.65/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.coredns"}, InterfaceName:"cali39ccf043b1d", MAC:"ba:1d:6b:93:6a:90", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"dns", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"UDP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"dns-tcp", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x23c1, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"liveness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f90, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"readiness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1ff5, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.142 [INFO][48708] cni-plugin/k8s.go 532: Wrote updated endpoint to datastore ContainerID="bab74d5f37b2692bb7e14970d595cad6ff7bab6a6228101343a3a04c78fad070" Namespace="kube-system" Pod="coredns-66bc5c9577-vl47p" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--vl47p-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.081 [INFO][48705] cni-plugin/utils.go 100: File /var/lib/calico/mtu does not exist
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.090 [INFO][48705] cni-plugin/plugin.go 340: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {minikube-k8s-coredns--66bc5c9577--mrrj7-eth0 coredns-66bc5c9577- kube-system  45969354-a71b-4150-b8bb-9fd34d203d83 775 0 2025-10-28 18:20:30 +0000 UTC <nil> <nil> map[k8s-app:kube-dns pod-template-hash:66bc5c9577 projectcalico.org/namespace:kube-system projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:coredns] map[] [] [] []} {k8s  minikube  coredns-66bc5c9577-mrrj7 eth0 coredns [] []   [kns.kube-system ksa.kube-system.coredns] caliae48a901644  [{dns UDP 53 0 } {dns-tcp TCP 53 0 } {metrics TCP 9153 0 } {liveness-probe TCP 8080 0 } {readiness-probe TCP 8181 0 }] [] <nil>}} ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.090 [INFO][48705] cni-plugin/k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.105 [INFO][48767] ipam/ipam_plugin.go 225: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" HandleID="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Workload="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.105 [INFO][48767] ipam/ipam_plugin.go 265: Auto assigning IP ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" HandleID="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Workload="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0xc0002e9070), Attrs:map[string]string{"namespace":"kube-system", "node":"minikube", "pod":"coredns-66bc5c9577-mrrj7", "timestamp":"2025-10-28 18:23:29.105632019 +0000 UTC"}, Hostname:"minikube", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.105 [INFO][48767] ipam/ipam_plugin.go 353: About to acquire host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.126 [INFO][48767] ipam/ipam_plugin.go 368: Acquired host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.126 [INFO][48767] ipam/ipam.go 110: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'minikube'
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.210 [INFO][48767] ipam/ipam.go 691: Looking up existing affinities for host handle="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.215 [INFO][48767] ipam/ipam.go 394: Looking up existing affinities for host host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.219 [INFO][48767] ipam/ipam.go 511: Trying affinity for 10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.221 [INFO][48767] ipam/ipam.go 158: Attempting to load block cidr=10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.224 [INFO][48767] ipam/ipam.go 235: Affinity is confirmed and block has been loaded cidr=10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.224 [INFO][48767] ipam/ipam.go 1220: Attempting to assign 1 addresses from block block=10.244.120.64/26 handle="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.225 [INFO][48767] ipam/ipam.go 1764: Creating new handle: k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.230 [INFO][48767] ipam/ipam.go 1243: Writing block in order to claim IPs block=10.244.120.64/26 handle="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48767] ipam/ipam.go 1256: Successfully claimed IPs: [10.244.120.66/26] block=10.244.120.64/26 handle="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48767] ipam/ipam.go 878: Auto-assigned 1 out of 1 IPv4s: [10.244.120.66/26] handle="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48767] ipam/ipam_plugin.go 374: Released host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48767] ipam/ipam_plugin.go 283: Calico CNI IPAM assigned addresses IPv4=[10.244.120.66/26] IPv6=[] ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" HandleID="k8s-pod-network.31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Workload="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.239 [INFO][48705] cni-plugin/k8s.go 418: Populated endpoint ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-coredns--66bc5c9577--mrrj7-eth0", GenerateName:"coredns-66bc5c9577-", Namespace:"kube-system", SelfLink:"", UID:"45969354-a71b-4150-b8bb-9fd34d203d83", ResourceVersion:"775", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-dns", "pod-template-hash":"66bc5c9577", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"coredns"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"", Pod:"coredns-66bc5c9577-mrrj7", Endpoint:"eth0", ServiceAccountName:"coredns", IPNetworks:[]string{"10.244.120.66/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.coredns"}, InterfaceName:"caliae48a901644", MAC:"", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"dns", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"UDP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"dns-tcp", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x23c1, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"liveness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f90, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"readiness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1ff5, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.239 [INFO][48705] cni-plugin/k8s.go 419: Calico CNI using IPs: [10.244.120.66/32] ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.239 [INFO][48705] cni-plugin/dataplane_linux.go 69: Setting the host side veth name to caliae48a901644 ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.243 [INFO][48705] cni-plugin/dataplane_linux.go 508: Disabling IPv4 forwarding ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.244 [INFO][48705] cni-plugin/k8s.go 446: Added Mac, interface name, and active container ID to endpoint ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-coredns--66bc5c9577--mrrj7-eth0", GenerateName:"coredns-66bc5c9577-", Namespace:"kube-system", SelfLink:"", UID:"45969354-a71b-4150-b8bb-9fd34d203d83", ResourceVersion:"775", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-dns", "pod-template-hash":"66bc5c9577", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"coredns"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4", Pod:"coredns-66bc5c9577-mrrj7", Endpoint:"eth0", ServiceAccountName:"coredns", IPNetworks:[]string{"10.244.120.66/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.coredns"}, InterfaceName:"caliae48a901644", MAC:"16:96:d0:be:2e:64", Ports:[]v3.WorkloadEndpointPort{v3.WorkloadEndpointPort{Name:"dns", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"UDP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"dns-tcp", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x35, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"metrics", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x23c1, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"liveness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1f90, HostPort:0x0, HostIP:""}, v3.WorkloadEndpointPort{Name:"readiness-probe", Protocol:numorstring.Protocol{Type:1, NumVal:0x0, StrVal:"TCP"}, Port:0x1ff5, HostPort:0x0, HostIP:""}}, AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.258 [INFO][48705] cni-plugin/k8s.go 532: Wrote updated endpoint to datastore ContainerID="31d7dd2f3cf44f70f1342b0fb294a1a71c8d61c2d4c2e6706730926d222bf4c4" Namespace="kube-system" Pod="coredns-66bc5c9577-mrrj7" WorkloadEndpoint="minikube-k8s-coredns--66bc5c9577--mrrj7-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.081 [INFO][48709] cni-plugin/utils.go 100: File /var/lib/calico/mtu does not exist
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.091 [INFO][48709] cni-plugin/plugin.go 340: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0 calico-kube-controllers-59556d9b4c- kube-system  7cfd42d8-6075-42d3-8452-e214170f6c10 776 0 2025-10-28 18:20:30 +0000 UTC <nil> <nil> map[k8s-app:calico-kube-controllers pod-template-hash:59556d9b4c projectcalico.org/namespace:kube-system projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:calico-kube-controllers] map[] [] [] []} {k8s  minikube  calico-kube-controllers-59556d9b4c-mmpxq eth0 calico-kube-controllers [] []   [kns.kube-system ksa.kube-system.calico-kube-controllers] cali44a3595e1ac  [] [] <nil>}} ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.091 [INFO][48709] cni-plugin/k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.106 [INFO][48769] ipam/ipam_plugin.go 225: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" HandleID="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Workload="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.106 [INFO][48769] ipam/ipam_plugin.go 265: Auto assigning IP ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" HandleID="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Workload="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0xc00004f6c0), Attrs:map[string]string{"namespace":"kube-system", "node":"minikube", "pod":"calico-kube-controllers-59556d9b4c-mmpxq", "timestamp":"2025-10-28 18:23:29.1064825 +0000 UTC"}, Hostname:"minikube", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.106 [INFO][48769] ipam/ipam_plugin.go 353: About to acquire host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48769] ipam/ipam_plugin.go 368: Acquired host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.236 [INFO][48769] ipam/ipam.go 110: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'minikube'
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.311 [INFO][48769] ipam/ipam.go 691: Looking up existing affinities for host handle="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.317 [INFO][48769] ipam/ipam.go 394: Looking up existing affinities for host host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.322 [INFO][48769] ipam/ipam.go 511: Trying affinity for 10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.324 [INFO][48769] ipam/ipam.go 158: Attempting to load block cidr=10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.326 [INFO][48769] ipam/ipam.go 235: Affinity is confirmed and block has been loaded cidr=10.244.120.64/26 host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.328 [INFO][48769] ipam/ipam.go 1220: Attempting to assign 1 addresses from block block=10.244.120.64/26 handle="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.329 [INFO][48769] ipam/ipam.go 1764: Creating new handle: k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.335 [INFO][48769] ipam/ipam.go 1243: Writing block in order to claim IPs block=10.244.120.64/26 handle="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.339 [INFO][48769] ipam/ipam.go 1256: Successfully claimed IPs: [10.244.120.67/26] block=10.244.120.64/26 handle="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.339 [INFO][48769] ipam/ipam.go 878: Auto-assigned 1 out of 1 IPv4s: [10.244.120.67/26] handle="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" host="minikube"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.339 [INFO][48769] ipam/ipam_plugin.go 374: Released host-wide IPAM lock.
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.339 [INFO][48769] ipam/ipam_plugin.go 283: Calico CNI IPAM assigned addresses IPv4=[10.244.120.67/26] IPv6=[] ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" HandleID="k8s-pod-network.e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Workload="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.341 [INFO][48709] cni-plugin/k8s.go 418: Populated endpoint ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0", GenerateName:"calico-kube-controllers-59556d9b4c-", Namespace:"kube-system", SelfLink:"", UID:"7cfd42d8-6075-42d3-8452-e214170f6c10", ResourceVersion:"776", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"calico-kube-controllers", "pod-template-hash":"59556d9b4c", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"calico-kube-controllers"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"", Pod:"calico-kube-controllers-59556d9b4c-mmpxq", Endpoint:"eth0", ServiceAccountName:"calico-kube-controllers", IPNetworks:[]string{"10.244.120.67/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.calico-kube-controllers"}, InterfaceName:"cali44a3595e1ac", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.341 [INFO][48709] cni-plugin/k8s.go 419: Calico CNI using IPs: [10.244.120.67/32] ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.341 [INFO][48709] cni-plugin/dataplane_linux.go 69: Setting the host side veth name to cali44a3595e1ac ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.342 [INFO][48709] cni-plugin/dataplane_linux.go 508: Disabling IPv4 forwarding ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.342 [INFO][48709] cni-plugin/k8s.go 446: Added Mac, interface name, and active container ID to endpoint ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0", GenerateName:"calico-kube-controllers-59556d9b4c-", Namespace:"kube-system", SelfLink:"", UID:"7cfd42d8-6075-42d3-8452-e214170f6c10", ResourceVersion:"776", Generation:0, CreationTimestamp:time.Date(2025, time.October, 28, 18, 20, 30, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"calico-kube-controllers", "pod-template-hash":"59556d9b4c", "projectcalico.org/namespace":"kube-system", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"calico-kube-controllers"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e", Pod:"calico-kube-controllers-59556d9b4c-mmpxq", Endpoint:"eth0", ServiceAccountName:"calico-kube-controllers", IPNetworks:[]string{"10.244.120.67/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.kube-system", "ksa.kube-system.calico-kube-controllers"}, InterfaceName:"cali44a3595e1ac", MAC:"36:ab:2b:df:1d:c0", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Oct 28 18:23:29 minikube cri-dockerd[1471]: 2025-10-28 18:23:29.347 [INFO][48709] cni-plugin/k8s.go 532: Wrote updated endpoint to datastore ContainerID="e9227a21b9017dcff3e6fb068d0603b132e7bd3c47ca1ba3b64963b83246374e" Namespace="kube-system" Pod="calico-kube-controllers-59556d9b4c-mmpxq" WorkloadEndpoint="minikube-k8s-calico--kube--controllers--59556d9b4c--mmpxq-eth0"


==> container status <==
CONTAINER           IMAGE                                                                                 CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
35547867f32f0       df191a54fb79d                                                                         13 minutes ago      Running             calico-kube-controllers   5                   e9227a21b9017       calico-kube-controllers-59556d9b4c-mmpxq
a091473354e03       52546a367cc9e                                                                         13 minutes ago      Running             coredns                   5                   31d7dd2f3cf44       coredns-66bc5c9577-mrrj7
074d6f282d3ad       52546a367cc9e                                                                         13 minutes ago      Running             coredns                   5                   bab74d5f37b26       coredns-66bc5c9577-vl47p
fe40db7c8fe95       ce9c4ac0f175f                                                                         13 minutes ago      Running             calico-node               0                   c8fef3e72a6fd       calico-node-wvw4m
d468751f6f8a0       calico/node@sha256:92d8bcca3280cd27b9c98cb6e70c3af10ad6ff8accd288919b04ae0cd6021c2e   13 minutes ago      Exited              mount-bpffs               0                   c8fef3e72a6fd       calico-node-wvw4m
31ba06d2b9564       034822460c2f6                                                                         15 minutes ago      Exited              install-cni               0                   c8fef3e72a6fd       calico-node-wvw4m
33817b4c4d39f       calico/cni@sha256:b32ac832411b188a8adc9e31b3e23cbbecd6d63c182a3802e947303f97c2f700    15 minutes ago      Exited              upgrade-ipam              0                   c8fef3e72a6fd       calico-node-wvw4m
701a1dcb4efb4       52546a367cc9e                                                                         15 minutes ago      Exited              coredns                   4                   ebf6d41c804aa       coredns-66bc5c9577-mrrj7
6f0eee0d98099       52546a367cc9e                                                                         15 minutes ago      Exited              coredns                   4                   ccbe3a2991259       coredns-66bc5c9577-vl47p
d55e93aeec6e4       df191a54fb79d                                                                         15 minutes ago      Exited              calico-kube-controllers   4                   f0050e537fc39       calico-kube-controllers-59556d9b4c-mmpxq
1be764b0eaaf7       df0860106674d                                                                         16 minutes ago      Running             kube-proxy                0                   e7ee604cf7560       kube-proxy-tbdt4
97ab179342359       46169d968e920                                                                         17 minutes ago      Running             kube-scheduler            0                   db15f32e20bfa       kube-scheduler-minikube
d50dc9f91414e       a0af72f2ec6d6                                                                         17 minutes ago      Running             kube-controller-manager   0                   fc122ab313e31       kube-controller-manager-minikube
163c870bf73ce       5f1f5298c888d                                                                         17 minutes ago      Running             etcd                      0                   31880e2814e42       etcd-minikube
3a48e83ce1e10       90550c43ad2bc                                                                         17 minutes ago      Running             kube-apiserver            0                   f244c19093420       kube-apiserver-minikube


==> coredns [074d6f282d3a] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [6f0eee0d9809] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] plugin/health: Going into lameduck mode for 5s
[ERROR] plugin/errors: 2 2745977396263945102.3415793004406611688. HINFO: dial udp 192.168.49.1:53: connect: network is unreachable
[ERROR] plugin/errors: 2 2745977396263945102.3415793004406611688. HINFO: dial udp 192.168.49.1:53: connect: network is unreachable


==> coredns [701a1dcb4efb] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] plugin/health: Going into lameduck mode for 5s
[ERROR] plugin/errors: 2 4304192144742894814.2922581677122185466. HINFO: dial udp 192.168.49.1:53: connect: network is unreachable
[ERROR] plugin/errors: 2 4304192144742894814.2922581677122185466. HINFO: dial udp 192.168.49.1:53: connect: network is unreachable


==> coredns [a091473354e0] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_28T23_50_26_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.49.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.120.64
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 28 Oct 2025 18:20:22 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 28 Oct 2025 18:37:14 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Tue, 28 Oct 2025 18:23:28 +0000   Tue, 28 Oct 2025 18:23:28 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Tue, 28 Oct 2025 18:37:14 +0000   Tue, 28 Oct 2025 18:20:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Tue, 28 Oct 2025 18:37:14 +0000   Tue, 28 Oct 2025 18:20:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Tue, 28 Oct 2025 18:37:14 +0000   Tue, 28 Oct 2025 18:20:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Tue, 28 Oct 2025 18:37:14 +0000   Tue, 28 Oct 2025 18:20:23 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  181293616Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24185752Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  181293616Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24185752Ki
  pods:               110
System Info:
  Machine ID:                 7ee38a669f264362bc82acda8869fb6d
  System UUID:                901c0778-8f5c-4f78-bfac-fdb54a3c16ff
  Boot ID:                    fc1ffbc2-9c87-49b0-9c32-d7bc55545607
  Kernel Version:             6.14.0-33-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-59556d9b4c-mmpxq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 calico-node-wvw4m                           250m (1%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 coredns-66bc5c9577-mrrj7                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     16m
  kube-system                 coredns-66bc5c9577-vl47p                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     16m
  kube-system                 etcd-minikube                               100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         16m
  kube-system                 kube-apiserver-minikube                     250m (1%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 kube-controller-manager-minikube            200m (1%)     0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 kube-proxy-tbdt4                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m
  kube-system                 kube-scheduler-minikube                     100m (0%)     0 (0%)      0 (0%)           0 (0%)         16m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1100m (6%)  0 (0%)
  memory             240Mi (1%)  340Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 16m                kube-proxy       
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m (x7 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 16m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  16m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  16m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    16m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     16m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           16m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct28 17:35] [Firmware Bug]: TSC ADJUST: CPU0: -955445803 force to 0
[  +0.000000] [Firmware Bug]: TSC ADJUST differs within socket(s), fixing all errors
[  +0.104289]   #1  #3  #5  #7  #9 #11
[  +0.762825] pnp 00:03: disabling [mem 0xc0000000-0xcfffffff] because it overlaps 0000:00:02.0 BAR 9 [mem 0x00000000-0xdfffffff 64bit pref]
[  +0.088664] hpet_acpi_add: no address or irqs in _CRS
[  +0.021793] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.007389] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.011353] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.421578] i2c_hid_acpi i2c-ASUP1200:00: device returned incorrect report (9 vs 2 expected)
[  +0.000006] hid-multitouch 0018:093A:200A.0001: failed to fetch feature 2
[  +1.610491] ACPI Warning: \_SB.IETM._TRT: Return Package has no elements (empty) (20240827/nsprepkg-94)
[  +0.212347] spi-nor spi0.0: supply vcc not found, using dummy regulator
[  +0.292838] asus_wmi: fan_curve_get_factory_default (0x00110024) failed: -19
[  +0.000236] asus_wmi: fan_curve_get_factory_default (0x00110025) failed: -19
[  +0.000237] asus_wmi: fan_curve_get_factory_default (0x00110032) failed: -19
[  +0.648880] skl_hda_dsp_generic skl_hda_dsp_generic: ASoC: Parent card not yet available, widget card binding deferred
[  +0.069957] skl_hda_dsp_generic skl_hda_dsp_generic: hda_dsp_hdmi_build_controls: no PCM in topology for HDMI converter 3
[  +0.090629] nvme nvme0: using unchecked data buffer
[  +0.014332] block nvme0n1: No UUID available providing old NGUID
[  +6.778389] kauditd_printk_skb: 175 callbacks suppressed
[  +2.326169] warning: `gnome-shell' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211


==> etcd [163c870bf73c] <==
{"level":"warn","ts":"2025-10-28T18:20:22.151740Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.160077Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58604","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.166819Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58620","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.173692Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58630","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.180477Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58650","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.188045Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58666","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.197496Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58684","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.204719Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58700","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.210435Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58720","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.216537Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58752","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.222265Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.231043Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58802","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.237971Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58816","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.244807Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58842","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.251416Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58846","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.258351Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.266214Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58898","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.272841Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58912","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.279699Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58930","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.297191Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58956","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.302143Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58966","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.319507Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59002","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.326006Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59014","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.332833Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59024","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.339732Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59064","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.354067Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59088","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.360753Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59102","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.367208Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.373514Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59136","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.398854Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59156","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.405471Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59174","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.412622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59188","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:22.446809Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.564649Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45504","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.586536Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45538","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.594077Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45568","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.602490Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45596","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.612203Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.624000Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45622","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.635054Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45642","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.647571Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.656249Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45686","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.667723Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45710","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.711039Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45732","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.729126Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45760","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.743544Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45780","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.750266Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45796","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.757700Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45824","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.762232Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45834","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.776943Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45868","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.785315Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45892","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.794520Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45904","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.803761Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45916","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-28T18:20:29.812714Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45924","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-10-28T18:30:21.981329Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":844}
{"level":"info","ts":"2025-10-28T18:30:21.989082Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":844,"took":"7.237077ms","hash":1900363545,"current-db-size-bytes":5259264,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":5259264,"current-db-size-in-use":"5.3 MB"}
{"level":"info","ts":"2025-10-28T18:30:21.989117Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1900363545,"revision":844,"compact-revision":-1}
{"level":"info","ts":"2025-10-28T18:35:21.989832Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":934}
{"level":"info","ts":"2025-10-28T18:35:21.992545Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":934,"took":"2.170661ms","hash":4179319382,"current-db-size-bytes":5259264,"current-db-size":"5.3 MB","current-db-size-in-use-bytes":1572864,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-10-28T18:35:21.992592Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4179319382,"revision":934,"compact-revision":844}


==> kernel <==
 18:37:20 up  1:02,  0 users,  load average: 0.70, 0.74, 0.73
Linux minikube 6.14.0-33-generic #33~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 19 17:02:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [3a48e83ce1e1] <==
I1028 18:20:26.172184       1 handler.go:285] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I1028 18:20:26.185923       1 handler.go:285] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I1028 18:20:26.187162       1 handler.go:285] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I1028 18:20:26.206111       1 handler.go:285] Adding GroupVersion policy.networking.k8s.io v1alpha1 to ResourceManager
I1028 18:20:26.236773       1 handler.go:285] Adding GroupVersion policy.networking.k8s.io v1alpha1 to ResourceManager
W1028 18:20:29.564565       1 logging.go:55] [core] [Channel #259 SubChannel #260]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.576257       1 logging.go:55] [core] [Channel #263 SubChannel #264]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.586492       1 logging.go:55] [core] [Channel #267 SubChannel #268]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.594008       1 logging.go:55] [core] [Channel #271 SubChannel #272]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.602507       1 logging.go:55] [core] [Channel #275 SubChannel #276]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.612199       1 logging.go:55] [core] [Channel #279 SubChannel #280]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.623959       1 logging.go:55] [core] [Channel #283 SubChannel #284]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.634999       1 logging.go:55] [core] [Channel #287 SubChannel #288]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.647545       1 logging.go:55] [core] [Channel #291 SubChannel #292]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.656229       1 logging.go:55] [core] [Channel #295 SubChannel #296]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.667667       1 logging.go:55] [core] [Channel #299 SubChannel #300]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.689474       1 logging.go:55] [core] [Channel #303 SubChannel #304]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.710953       1 logging.go:55] [core] [Channel #307 SubChannel #308]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.729101       1 logging.go:55] [core] [Channel #311 SubChannel #312]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.743494       1 logging.go:55] [core] [Channel #315 SubChannel #316]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.750262       1 logging.go:55] [core] [Channel #319 SubChannel #320]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.757712       1 logging.go:55] [core] [Channel #323 SubChannel #324]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1028 18:20:29.762239       1 logging.go:55] [core] [Channel #327 SubChannel #328]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.769563       1 logging.go:55] [core] [Channel #331 SubChannel #332]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.776899       1 logging.go:55] [core] [Channel #335 SubChannel #336]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.785316       1 logging.go:55] [core] [Channel #339 SubChannel #340]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.794499       1 logging.go:55] [core] [Channel #343 SubChannel #344]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.803772       1 logging.go:55] [core] [Channel #347 SubChannel #348]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1028 18:20:29.812664       1 logging.go:55] [core] [Channel #351 SubChannel #352]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1028 18:20:30.312463       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1028 18:20:30.316217       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1028 18:20:30.606861       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1028 18:20:30.669698       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1028 18:21:34.194954       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:21:50.961395       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:23:00.729205       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:23:02.427441       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:24:05.118073       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:24:07.805973       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:25:18.620008       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:25:32.936572       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:26:22.615067       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:26:58.613833       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:27:49.801628       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:28:02.833195       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:28:58.707702       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:29:24.382029       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:30:21.735472       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:30:22.656647       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1028 18:30:48.800244       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:31:29.013652       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:31:49.019255       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:32:46.593049       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:32:50.074110       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:34:01.419788       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:34:13.437581       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:35:18.568263       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:35:41.027209       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:36:19.986955       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1028 18:36:48.243966       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [d50dc9f91414] <==
I1028 18:20:29.552521       1 controllermanager.go:781] "Started controller" controller="taint-eviction-controller"
I1028 18:20:29.552630       1 taint_eviction.go:282] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
I1028 18:20:29.552821       1 taint_eviction.go:288] "Sending events to api server" logger="taint-eviction-controller"
I1028 18:20:29.552874       1 shared_informer.go:349] "Waiting for caches to sync" controller="taint-eviction-controller"
I1028 18:20:29.558981       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1028 18:20:29.566679       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1028 18:20:29.572093       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1028 18:20:29.573679       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1028 18:20:29.588088       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1028 18:20:29.597106       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1028 18:20:29.601220       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1028 18:20:29.601323       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1028 18:20:29.601448       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1028 18:20:29.601602       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1028 18:20:29.601647       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1028 18:20:29.601857       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1028 18:20:29.603670       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1028 18:20:29.603744       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1028 18:20:29.603785       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1028 18:20:29.603793       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1028 18:20:29.603809       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1028 18:20:29.605078       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1028 18:20:29.605338       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1028 18:20:29.606266       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1028 18:20:29.606276       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1028 18:20:29.607405       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1028 18:20:29.607521       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1028 18:20:29.608304       1 shared_informer.go:356] "Caches are synced" controller="job"
I1028 18:20:29.608313       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1028 18:20:29.609637       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1028 18:20:29.613487       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1028 18:20:29.652147       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1028 18:20:29.652158       1 shared_informer.go:356] "Caches are synced" controller="node"
I1028 18:20:29.652168       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1028 18:20:29.652205       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1028 18:20:29.652216       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1028 18:20:29.652254       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1028 18:20:29.652272       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1028 18:20:29.652278       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1028 18:20:29.652284       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1028 18:20:29.652390       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1028 18:20:29.653616       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1028 18:20:29.653626       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1028 18:20:29.655046       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1028 18:20:29.655121       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1028 18:20:29.656176       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1028 18:20:29.657406       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1028 18:20:29.657410       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1028 18:20:29.657771       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1028 18:20:29.657901       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1028 18:20:29.658746       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1028 18:20:29.658890       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1028 18:20:29.662149       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1028 18:20:29.664808       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1028 18:20:30.664551       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1028 18:20:30.666600       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1028 18:20:30.874354       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1028 18:20:30.903054       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1028 18:20:30.903082       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1028 18:20:30.903093       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [1be764b0eaaf] <==
I1028 18:20:31.850869       1 server_linux.go:53] "Using iptables proxy"
I1028 18:20:31.947308       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1028 18:20:32.048241       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1028 18:20:32.048381       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1028 18:20:32.048669       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1028 18:20:32.080102       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1028 18:20:32.080205       1 server_linux.go:132] "Using iptables Proxier"
I1028 18:20:32.086449       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1028 18:20:32.093269       1 server.go:527] "Version info" version="v1.34.0"
I1028 18:20:32.093285       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1028 18:20:32.094407       1 config.go:200] "Starting service config controller"
I1028 18:20:32.094431       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1028 18:20:32.094465       1 config.go:309] "Starting node config controller"
I1028 18:20:32.094474       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1028 18:20:32.094467       1 config.go:106] "Starting endpoint slice config controller"
I1028 18:20:32.094511       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1028 18:20:32.094544       1 config.go:403] "Starting serviceCIDR config controller"
I1028 18:20:32.094586       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1028 18:20:32.194569       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1028 18:20:32.194612       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1028 18:20:32.194692       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1028 18:20:32.194719       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [97ab17934235] <==
I1028 18:20:21.021406       1 serving.go:386] Generated self-signed cert in-memory
W1028 18:20:22.644805       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1028 18:20:22.644850       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1028 18:20:22.644868       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1028 18:20:22.644881       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1028 18:20:22.659092       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1028 18:20:22.659104       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1028 18:20:22.660204       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1028 18:20:22.660222       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1028 18:20:22.660278       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1028 18:20:22.660997       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1028 18:20:22.661705       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1028 18:20:22.661705       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1028 18:20:22.661706       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1028 18:20:22.662019       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1028 18:20:22.662083       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1028 18:20:22.662080       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1028 18:20:22.662095       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1028 18:20:22.662150       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1028 18:20:22.662280       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1028 18:20:22.662310       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1028 18:20:22.662319       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1028 18:20:22.662345       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1028 18:20:22.662361       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1028 18:20:22.662368       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1028 18:20:22.662377       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1028 18:20:22.662379       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1028 18:20:22.662386       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1028 18:20:22.662400       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1028 18:20:22.662420       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1028 18:20:23.492843       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1028 18:20:23.533195       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1028 18:20:23.638737       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1028 18:20:23.650851       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1028 18:20:23.652887       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1028 18:20:23.679240       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1028 18:20:23.835629       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1028 18:20:23.880880       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1028 18:20:23.882735       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1028 18:20:23.896812       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1028 18:20:23.903971       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1028 18:20:23.952194       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
I1028 18:20:26.161066       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.705535    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fd0df07b1c3ebe30a3a6fcc2f278da7b65b5fea30af4e7dc4ff1409d407fff3f\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.705577    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fd0df07b1c3ebe30a3a6fcc2f278da7b65b5fea30af4e7dc4ff1409d407fff3f\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.705591    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fd0df07b1c3ebe30a3a6fcc2f278da7b65b5fea30af4e7dc4ff1409d407fff3f\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.705628    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"fd0df07b1c3ebe30a3a6fcc2f278da7b65b5fea30af4e7dc4ff1409d407fff3f\\\" network for pod \\\"coredns-66bc5c9577-vl47p\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-vl47p_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-vl47p" podUID="8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.740166    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"9ff31a32fbca1cfc28eca271611e9c9a88e85bd387eb3a839616022be187694c\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.740208    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"9ff31a32fbca1cfc28eca271611e9c9a88e85bd387eb3a839616022be187694c\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.740233    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"9ff31a32fbca1cfc28eca271611e9c9a88e85bd387eb3a839616022be187694c\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.740272    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"9ff31a32fbca1cfc28eca271611e9c9a88e85bd387eb3a839616022be187694c\\\" network for pod \\\"coredns-66bc5c9577-mrrj7\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-mrrj7_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-mrrj7" podUID="45969354-a71b-4150-b8bb-9fd34d203d83"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.792797    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.792893    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.792924    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:24 minikube kubelet[2442]: E1028 18:23:24.793022    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500\\\" network for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq\\\": networkPlugin cni failed to set up pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq" podUID="7cfd42d8-6075-42d3-8452-e214170f6c10"
Oct 28 18:23:25 minikube kubelet[2442]: I1028 18:23:25.634430    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fd0df07b1c3ebe30a3a6fcc2f278da7b65b5fea30af4e7dc4ff1409d407fff3f"
Oct 28 18:23:25 minikube kubelet[2442]: I1028 18:23:25.682717    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9ff31a32fbca1cfc28eca271611e9c9a88e85bd387eb3a839616022be187694c"
Oct 28 18:23:25 minikube kubelet[2442]: I1028 18:23:25.723572    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.868013    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"5a673fe1443c97236c6da49babea34bfda867a480f882f45b58f87cc4506ce03\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.868064    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"5a673fe1443c97236c6da49babea34bfda867a480f882f45b58f87cc4506ce03\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.868084    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"5a673fe1443c97236c6da49babea34bfda867a480f882f45b58f87cc4506ce03\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.868133    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"5a673fe1443c97236c6da49babea34bfda867a480f882f45b58f87cc4506ce03\\\" network for pod \\\"coredns-66bc5c9577-vl47p\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-vl47p_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-vl47p" podUID="8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.901064    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"81660050d17f94ce6129844d91d9ae0b310f313027adf6a67e65f0f2f394017f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.901115    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"81660050d17f94ce6129844d91d9ae0b310f313027adf6a67e65f0f2f394017f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.901134    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"81660050d17f94ce6129844d91d9ae0b310f313027adf6a67e65f0f2f394017f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.901176    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"81660050d17f94ce6129844d91d9ae0b310f313027adf6a67e65f0f2f394017f\\\" network for pod \\\"coredns-66bc5c9577-mrrj7\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-mrrj7_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-mrrj7" podUID="45969354-a71b-4150-b8bb-9fd34d203d83"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.937455    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"f83e48f1d339ccf447edc2588fcca1a6b2eb02e7c05035b0f808eed79c03845d\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.937515    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"f83e48f1d339ccf447edc2588fcca1a6b2eb02e7c05035b0f808eed79c03845d\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.937544    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"f83e48f1d339ccf447edc2588fcca1a6b2eb02e7c05035b0f808eed79c03845d\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:25 minikube kubelet[2442]: E1028 18:23:25.937618    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"f83e48f1d339ccf447edc2588fcca1a6b2eb02e7c05035b0f808eed79c03845d\\\" network for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq\\\": networkPlugin cni failed to set up pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq" podUID="7cfd42d8-6075-42d3-8452-e214170f6c10"
Oct 28 18:23:26 minikube kubelet[2442]: I1028 18:23:26.731358    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5a673fe1443c97236c6da49babea34bfda867a480f882f45b58f87cc4506ce03"
Oct 28 18:23:26 minikube kubelet[2442]: I1028 18:23:26.735831    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="81660050d17f94ce6129844d91d9ae0b310f313027adf6a67e65f0f2f394017f"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.739134    2442 kuberuntime_manager.go:1744] "PodSandboxStatus of sandbox for pod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500" podSandboxID="af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.739165    2442 generic.go:455] "PLEG: Write status" err="rpc error: code = Unknown desc = Error response from daemon: No such container: af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.739187    2442 generic.go:300] "PLEG: Ignoring events for pod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.739248    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="rpc error: code = Unknown desc = Error response from daemon: No such container: af01d33c75b1796c65396d03a44b5c774d3fe0b682811851aaadc01f61b58500" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq" podUID="7cfd42d8-6075-42d3-8452-e214170f6c10"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.984110    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d625c6b233b72072dcd4687564e4495739cb79c45c73fac80077ce23b742fc4f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.984150    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d625c6b233b72072dcd4687564e4495739cb79c45c73fac80077ce23b742fc4f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.984163    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d625c6b233b72072dcd4687564e4495739cb79c45c73fac80077ce23b742fc4f\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.984220    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"d625c6b233b72072dcd4687564e4495739cb79c45c73fac80077ce23b742fc4f\\\" network for pod \\\"coredns-66bc5c9577-mrrj7\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-mrrj7_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-mrrj7" podUID="45969354-a71b-4150-b8bb-9fd34d203d83"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.985144    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"cf89268d137434ebac66a211dcf2855df43400b8b5f061327dca1fee82e9740a\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.985170    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"cf89268d137434ebac66a211dcf2855df43400b8b5f061327dca1fee82e9740a\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.985181    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"cf89268d137434ebac66a211dcf2855df43400b8b5f061327dca1fee82e9740a\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:26 minikube kubelet[2442]: E1028 18:23:26.985213    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"cf89268d137434ebac66a211dcf2855df43400b8b5f061327dca1fee82e9740a\\\" network for pod \\\"coredns-66bc5c9577-vl47p\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-vl47p_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-vl47p" podUID="8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6"
Oct 28 18:23:27 minikube kubelet[2442]: I1028 18:23:27.748660    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="cf89268d137434ebac66a211dcf2855df43400b8b5f061327dca1fee82e9740a"
Oct 28 18:23:27 minikube kubelet[2442]: I1028 18:23:27.758183    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d625c6b233b72072dcd4687564e4495739cb79c45c73fac80077ce23b742fc4f"
Oct 28 18:23:27 minikube kubelet[2442]: I1028 18:23:27.765251    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f83e48f1d339ccf447edc2588fcca1a6b2eb02e7c05035b0f808eed79c03845d"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.054601    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fb3e7a6c31bd1db4da70e053372f41af3a1e80e27252721f64d5cefbb2334bc9\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.054678    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fb3e7a6c31bd1db4da70e053372f41af3a1e80e27252721f64d5cefbb2334bc9\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.054707    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"fb3e7a6c31bd1db4da70e053372f41af3a1e80e27252721f64d5cefbb2334bc9\" network for pod \"coredns-66bc5c9577-vl47p\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-vl47p_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-vl47p"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.054778    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-vl47p_kube-system(8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"fb3e7a6c31bd1db4da70e053372f41af3a1e80e27252721f64d5cefbb2334bc9\\\" network for pod \\\"coredns-66bc5c9577-vl47p\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-vl47p_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-vl47p" podUID="8d80f5aa-84d3-48ca-8a43-e6eba2fb13b6"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057431    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"e2f7074d0a66c32272070479402cbee0b427a645d7f2afee902607344025b454\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057492    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"e2f7074d0a66c32272070479402cbee0b427a645d7f2afee902607344025b454\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057526    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"e2f7074d0a66c32272070479402cbee0b427a645d7f2afee902607344025b454\" network for pod \"coredns-66bc5c9577-mrrj7\": networkPlugin cni failed to set up pod \"coredns-66bc5c9577-mrrj7_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-66bc5c9577-mrrj7"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057594    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-66bc5c9577-mrrj7_kube-system(45969354-a71b-4150-b8bb-9fd34d203d83)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"e2f7074d0a66c32272070479402cbee0b427a645d7f2afee902607344025b454\\\" network for pod \\\"coredns-66bc5c9577-mrrj7\\\": networkPlugin cni failed to set up pod \\\"coredns-66bc5c9577-mrrj7_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-66bc5c9577-mrrj7" podUID="45969354-a71b-4150-b8bb-9fd34d203d83"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057925    2442 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"283f561e5d240098033c76acb898d9533c5cd9e9f442087b64394e52ffbe5bde\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.057974    2442 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"283f561e5d240098033c76acb898d9533c5cd9e9f442087b64394e52ffbe5bde\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.058005    2442 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"283f561e5d240098033c76acb898d9533c5cd9e9f442087b64394e52ffbe5bde\" network for pod \"calico-kube-controllers-59556d9b4c-mmpxq\": networkPlugin cni failed to set up pod \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq"
Oct 28 18:23:28 minikube kubelet[2442]: E1028 18:23:28.058046    2442 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system(7cfd42d8-6075-42d3-8452-e214170f6c10)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"283f561e5d240098033c76acb898d9533c5cd9e9f442087b64394e52ffbe5bde\\\" network for pod \\\"calico-kube-controllers-59556d9b4c-mmpxq\\\": networkPlugin cni failed to set up pod \\\"calico-kube-controllers-59556d9b4c-mmpxq_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/calico-kube-controllers-59556d9b4c-mmpxq" podUID="7cfd42d8-6075-42d3-8452-e214170f6c10"
Oct 28 18:23:28 minikube kubelet[2442]: I1028 18:23:28.808480    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fb3e7a6c31bd1db4da70e053372f41af3a1e80e27252721f64d5cefbb2334bc9"
Oct 28 18:23:28 minikube kubelet[2442]: I1028 18:23:28.814697    2442 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/calico-node-wvw4m" podStartSLOduration=2.987908657 podStartE2EDuration="2m58.814677032s" podCreationTimestamp="2025-10-28 18:20:30 +0000 UTC" firstStartedPulling="2025-10-28 18:20:31.687887933 +0000 UTC m=+6.928451630" lastFinishedPulling="2025-10-28 18:23:27.51465631 +0000 UTC m=+182.755220005" observedRunningTime="2025-10-28 18:23:28.814517726 +0000 UTC m=+184.055081466" watchObservedRunningTime="2025-10-28 18:23:28.814677032 +0000 UTC m=+184.055240728"
Oct 28 18:23:28 minikube kubelet[2442]: I1028 18:23:28.819563    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e2f7074d0a66c32272070479402cbee0b427a645d7f2afee902607344025b454"
Oct 28 18:23:28 minikube kubelet[2442]: I1028 18:23:28.831230    2442 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="283f561e5d240098033c76acb898d9533c5cd9e9f442087b64394e52ffbe5bde"

